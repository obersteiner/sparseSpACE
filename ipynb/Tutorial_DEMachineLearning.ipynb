{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with the use of Density Estimation\n",
    "\n",
    "This notebook provides a quick summary and introduction on how to use the $\\texttt{DataSet}$, $\\texttt{Classification}$ and $\\texttt{Clustering}$ wrappers in the $\\texttt{DEMachineLearning}$ module.\n",
    "For a detailed explanation on the algorithms used in this implementation, please refer directly to the documentation of said module or the corresponding Bachelor's Thesis \"Machine Learning with the Sparse Grid Density Estimation using the Combination Technique\".\n",
    "\n",
    "Since most machine learning tasks operate on some sets of data, either for learning or testing purposes, the helper-class $\\texttt{DataSet}$ was created to assist in the construction of the $\\texttt{Classification}$ and $\\texttt{Clustering}$ classes, which implement the actual machine learning algorithms. The following three subsections explain those three classes accordingly in more detail.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataSet class\n",
    "\n",
    "The core functionality of $\\texttt{DataSet}$ is to store some data in a convenient and consistent way to perform various tasks with (here only, but not limited to classification and clustering). Therefore the actual data is stored within the protected attribute $\\texttt{data}$ in the form of a tuple with two ndarray-entries, whereas the first entry represents the actual samples and the second one the corresponding labels. This $\\texttt{data}$ is initialized with a necessary $\\texttt{raw_data}$ constructor parameter, along with some other optional ones, which set the $\\texttt{name}$ or $\\texttt{label}$-type of a $\\texttt{DataSet}$ object.\n",
    "The $\\texttt{raw_data}$ parameter will always be parsed into the previously described form of a 2-length-tuple, but can be passed to the constructor in two ways:\n",
    "+ a $n$-dimensional ndarray with type $\\texttt{float64}$\n",
    "+ a 2-dimensional tuple with its first entry being a $n$-dimensional ndarray of size $m$ with type $\\texttt{float64}$ and its second entry being a 2-dimensional ndarray of size $m$ with type $\\texttt{int64}$\n",
    "\n",
    "In the following code example, this $\\texttt{raw_data}$ parameter will be generated or loaded with the help of the $\\textit{scikit-learn library}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparseSpACE\n",
    "import sparseSpACE.DEMachineLearning as deml\n",
    "\n",
    "\n",
    "# generate a data set of size \"samples\", features \"dimension\" and classes \"labels\" with the sklearn library\n",
    "samples = 500\n",
    "dimension = 2\n",
    "labels = 6\n",
    "# sklearn_dataset = deml.datasets.make_circles(n_samples=samples, noise=0.05)\n",
    "sklearn_dataset = deml.datasets.make_moons(n_samples=samples, noise=0.15)\n",
    "# sklearn_dataset = deml.datasets.make_classification(n_samples=samples, n_features=dimension, n_redundant=0, n_clusters_per_class=1, n_informative=2, n_classes=(labels if labels < 4 else 4))\n",
    "# sklearn_dataset = deml.datasets.make_blobs(n_samples=samples, n_features=dimension, centers=labels)\n",
    "# sklearn_dataset = deml.datasets.make_gaussian_quantiles(n_samples=samples, n_features=dimension, n_classes=labels)\n",
    "# sklearn_dataset = deml.datasets.load_digits(return_X_y=True) # hint: try only with max_level <= 3\n",
    "# sklearn_dataset = deml.datasets.load_iris(return_X_y=True)\n",
    "# sklearn_dataset = deml.datasets.load_breast_cancer(return_X_y=True) # hint: try only with max_level <= 4\n",
    "# sklearn_dataset = deml.datasets.load_wine(return_X_y=True)\n",
    "\n",
    "# now we can transform this dataset into a DataSet object and give it an appropriate name\n",
    "data = deml.DataSet(sklearn_dataset, name='Input_Set')\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# now let's look at some functions of the DataSet class\n",
    "\n",
    "# DataSet objects can e.g. be ...\n",
    "data_copy = data.copy()                                              # deepcopied\n",
    "data_copy.scale_range((0.005, 0.995))                                     # scaled\n",
    "part0, part1 = data_copy.split_pieces(0.5)                           # split\n",
    "data_copy = part0.concatenate(part1)                                 # concatenated\n",
    "data_copy.set_name('2nd_Set')                                        # renamed\n",
    "data_copy.remove_labels(0.5)                                         # freed of some label assignments to samples\n",
    "without_labels, with_labels = data_copy.split_without_labels()       # seperated into samples with and without classes\n",
    "data_copy.plot()                                                     # plotted\n",
    "\n",
    "# and of course a regular density estimation can also be performed on a DataSet object:\n",
    "de_retval = data_copy.density_estimation(plot_de_dataset=False, plot_sparsegrid=False, plot_density_estimation=True, plot_combi_scheme=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### The Classification class\n",
    "\n",
    "Classification tasks can be performed on previously created $\\texttt{DataSet}$ objects. Therefore this process is divided into the key steps $\\textit{initialization}$, $\\textit{perform learning}$ and $\\textit{evaluation}$.\n",
    "\n",
    "In the initialization step, the constructor of the $\\texttt{Classification}$ class is called with one necessary data parameter and some optional parameters:\n",
    "+ The initial $\\texttt{DataSet}$ on which to perform the classification task.\n",
    "+ An optional explicitly stated data range of the data set.\n",
    "+ A percentage factor, which specifies how much of the input data set should be used for learning. The rest will be used for testing later on.\n",
    "+ A boolean value, which specifies whether the learning data should consist of evenly or randomly sized classes (not important if 0 < percentage < 1).\n",
    "+ Another boolean value, which specifies whether the data should be shuffled to ensure a random distrubution of samples before the learning is performed.\n",
    "\n",
    "Learning is performed based on the $\\texttt{DensityEstimation}$ class in the $\\texttt{GridOperation}$ module. So the learning process can either be done regularly with $\\texttt{perform_classification}$ or dimension wise with $\\texttt{perform_classification_dimension_wise}$. The input parameters for those functions are those of the underlying density estimation plus a boolean value, which specifies whether learning performance metrics should be printed. Note that learning can only performed once on a $\\texttt{Classification}$ object.\n",
    "\n",
    "There are various options for evaluating the results of a performed classification learning task. Some are presented in the following code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Classification object with the original unedited data\n",
    "# 80% of this data is going to be used as learning part with equally distributed classes\n",
    "classification = deml.Classification(data, split_percentage=0.8, split_evenly=True, shuffle_data=True)\n",
    "\n",
    "# after that, the learning process of classification should be performed immediately, since no other method can be called before that without raising an error\n",
    "classification.perform_classification(masslumping=True, lambd=0.0, minimum_level=1, maximum_level=5, print_metrics=True)\n",
    "# classification.perform_classification_dimension_wise(masslumping=True, lambd=0.0, minimum_level=1, maximum_level=5, print_metrics=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# now some other operations can be performed on the Classification object and results can be evaluated\n",
    "\n",
    "# e.g. the object's classificators and corresponding density estimations could be plotted\n",
    "classification.plot(plot_class_sparsegrid=False, plot_class_combi_scheme=False, plot_class_dataset=True, plot_class_density_estimation=True)\n",
    "\n",
    "# if some testing data is already added to the Classification object (which here is the case, since 20% of samples are testing samples), its evaluation can be printed already\n",
    "evaluation = classification.evaluate()\n",
    "print(\"Percentage of correct mappings\",evaluation[\"Percentage correct\"])\n",
    "#classification.print_evaluation(print_incorrect_points=True)\n",
    "\n",
    "# also more testing data can be added and the results can be printed immediately\n",
    "with_labels.set_name(\"Test_new_data\")\n",
    "classification.test_data(with_labels, print_output=True)\n",
    "\n",
    "# and the Classification object can be called to perform blind classification on a dataset with unknown class assignments to its samples\n",
    "data_copy.remove_labels(1.0)\n",
    "calcult_classes = classification(data_copy)\n",
    "\n",
    "# if the used data set is two- or three-dimensional, the results can be plotted to easily see, which samples were classified correctly and which were not\n",
    "correct_classes = data.copy()\n",
    "correct_classes.scale_range((0.005, 0.995))\n",
    "correct_classes.set_name('Correct_Classes')\n",
    "calcult_classes.set_name('Calculated_Classes')\n",
    "retfig0 = correct_classes.plot()\n",
    "retfig1 = calcult_classes.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### The Clustering class\n",
    "\n",
    "Clustering is also performed on those previously created $\\texttt{DataSet}$ objects. To provide a similar interface, the individual process steps remain similar, while the algorithm itself is fundamentally different.\n",
    "\n",
    "In the initialization step, the constructor of the $\\texttt{Clustering}$ class is called with one necessary data parameter and some optional parameters:\n",
    "+ The initial $\\texttt{DataSet}$ on which to perform the clustering task.\n",
    "+ An integer value, which specifies the number of nearest neighbors for the connected graph.\n",
    "+ A percentage value, which specifies the threshold with which the edges of the nearest neighbor graph should be omitted.\n",
    "\n",
    "Learning is performed based on the $\\texttt{DensityEstimation}$ class in the $\\texttt{GridOperation}$ module. So the learning process can either be done regularly with $\\texttt{perform_clustering}$ or dimension wise with $\\texttt{perform_clustering_dimension_wise}$. The input parameters for those functions are those of the underlying density estimation plus a boolean value, which specifies whether learning performance metrics should be printed. Note that learning can only performed once on a $\\texttt{Clustering}$ object.\n",
    "\n",
    "After the performed learning process of a clustering task, results can be evaluated either by printing by by plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize Clustering object with the original unedited data, \n",
    "# the number of nearest neighbors for the initial nearest neighbor graph (before cutting the edges) and \n",
    "# an edge cutting threshold (edges with lower estimated density than the threshold will be cut)\n",
    "clus = deml.Clustering(data, number_nearest_neighbors=10, edge_cutting_threshold=0.6)\n",
    "\n",
    "# as with the classification task, the learning process of clustering should be performed immediately after initialization\n",
    "clus.perform_clustering(masslumping=False, lambd=10**-4, minimum_level=1, maximum_level=5, print_metrics=True)\n",
    "# classification.perform_clustering_dimension_wise(masslumping=True, lambd=0.0, minimum_level=1, maximum_level=5, print_metrics=True)\n",
    "\n",
    "# but different than for Classification, only the final results can be returned\n",
    "clus.print_evaluation(print_clusters=True)\n",
    "clus.plot(plot_original_dataset=True, plot_clustered_dataset=True, plot_cluster_density_estimation=True, plot_cluster_combi_scheme=False, plot_cluster_sparsegrid=False, plot_nearest_neighbor_graphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
